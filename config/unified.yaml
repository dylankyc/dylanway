# MyWay Unified Configuration
# This file combines both LLM provider settings and sink configurations

# OPENSEARCH STORAGE MODES:
#
# Configure storage behavior using the 'storage_mode' setting in opensearch sink config:
#
# A) COMBINED STORAGE (storage_mode: "combined"):
#    - Stores request and response in a single OpenSearch document
#    - Benefits: Easier querying, better data locality, reduced storage overhead
#    - Use cases: Analytics, reporting, simple visualization
#    - Document structure: {request_data: {...}, response_data: {...}, metadata: {...}, timestamp: "..."}
#    - Index: Uses single index (e.g., "llm-logs")
#
# B) SEPARATE STORAGE (storage_mode: "separate"):
#    - Stores request and response as separate OpenSearch documents
#    - Benefits: Real-time request tracking, detailed analysis, streaming support
#    - Use cases: Monitoring, debugging, real-time dashboards, error analysis
#    - Document structure: {doc_type: "request|response", data: {...}, metadata: {...}, timestamp: "..."}
#    - Index: Uses same index with doc_type field, or separate indices if configured
#
# To switch between modes:
# 1. Update opensearch.config.storage_mode in this file
# 2. Restart MyWay service: docker-compose restart myway-server
# 3. Update OpenSearch Dashboard index patterns if needed

# Global application settings
global:
  app_name: "myway"
  app_version: "0.1.0"

  # Server configuration
  server:
    host: "0.0.0.0"
    port: 9999
    cors: true
    request_timeout: 300 # seconds
    max_request_size: 10485760 # 10MB
    workers: 4

  # Logging configuration
  logging:
    level: "info" # trace, debug, info, warn, error
    format: "pretty" # json, pretty, compact
    structured: false
    # file: "/var/log/myway.log"  # Optional file output

  # Metrics configuration
  metrics:
    enabled: true
    azure_endpoint: "/metrics"
    prometheus: true

  # Rate limiting configuration
  rate_limiting:
    enabled: true
    backend: "redis"  # Options: "memory" or "redis"

    # Redis configuration (only used when backend is "redis")
    redis:
      url: "${REDIS_URL}"
      key_prefix: "myway:rate_limit:"
      connection_timeout: 5
      command_timeout: 2

    # Per-route rate limits
    routes:
      - route: "/v1/chat/completions"
        limit: 100
        window_secs: 3600  # 1 hour
        enabled: true
      - route: "/v1/embeddings"
        limit: 500
        window_secs: 3600  # 1 hour
        enabled: true
      - route: "/v1/models"
        limit: 1000
        window_secs: 3600  # 1 hour
        enabled: true

    # Default rate limit for routes not specifically configured
    default:
      limit: 200
      window_secs: 3600  # 1 hour
      enabled: true

# LLM Provider configurations
providers:
  # Global provider settings
  default_provider: "openai"
  timeout_seconds: 120
  max_retries: 3

  # OpenAI configuration
  openai:
    api_key: "${OPENAI_API_KEY}"
    # base_url: "https://api.openai.com/v1"  # Optional custom base URL
    timeout_seconds: 120
    max_retries: 3
    rate_limiting:
      limit: 100
      window_secs: 3600  # 1 hour
      enabled: true
      routes:
      - route: "/v1/chat/completions"
        limit: 100
        window_secs: 3600  # 1 hour
        enabled: true
    allowed_models:
      - "gpt-4o"
      - "gpt-4o-mini"
      - "gpt-5-mini"

  # Anthropic configuration
  anthropic:
    api_key: "${ANTHROPIC_API_KEY}"
    timeout_seconds: 30
    max_retries: 3
    rate_limiting:
      limit: 50
      window_secs: 3600  # 1 hour
      enabled: true

  # Google Gemini configuration
  gemini:
    api_key: "${GEMINI_API_KEY}"
    timeout_seconds: 30
    max_retries: 3
    rate_limiting:
      limit: 75
      window_secs: 3600  # 1 hour
      enabled: true

  # Cohere configuration
  cohere:
    api_key: "${COHERE_API_KEY}"
    timeout_seconds: 30
    rate_limiting:
      limit: 60
      window_secs: 3600  # 1 hour
      enabled: true

  # HuggingFace configuration
  huggingface:
    api_key: "${HUGGINGFACE_API_KEY}"
    base_url: "https://router.huggingface.co"
    rate_limiting:
      limit: 200
      window_secs: 3600  # 1 hour
      enabled: true

  # Mistral configuration
  mistral:
    api_key: "${MISTRAL_API_KEY}"
    rate_limiting:
      limit: 200
      window_secs: 3600  # 1 hour
      enabled: true

  # AWS Bedrock configuration
  bedrock:
    region: "us-east-1"
    access_key_id: "${AWS_ACCESS_KEY_ID}"
    secret_access_key: "${AWS_SECRET_ACCESS_KEY}"
    # access_key_id: "${AWS_BEDROCK_ACCESS_KEY_ID}"
    # secret_access_key: "${AWS_BEDROCK_SECRET_ACCESS_KEY}"
    # profile: "default"  # Use AWS profile instead of keys
    timeout_seconds: 60
    rate_limiting:
      limit: 80
      window_secs: 3600  # 1 hour
      enabled: true

  # Azure OpenAI configuration
  # Endpoint is auto-built using environment variables in priority order:
  # 1. AZURE_OPENAI_INSTANCE_NAME -> https://{instance_name}.openai.azure.com
  # 2. AZURE_OPENAI_RESOURCE_NAME -> https://{resource_name}.openai.azure.com
  # 3. AZURE_OPENAI_ENDPOINT (use as-is)
  # Note: Deployment names are specified via model parameter (e.g., "azure/gpt-4o")
  azure_openai:
    api_key: "${AZURE_OPENAI_API_KEY}"
    azure_instance_name: "${AZURE_OPENAI_INSTANCE_NAME}"
    azure_resource_name: "${AZURE_OPENAI_RESOURCE_NAME}"
    azure_endpoint: "${AZURE_OPENAI_ENDPOINT}"  # Only needed if not using AZURE_OPENAI_INSTANCE_NAME or AZURE_OPENAI_RESOURCE_NAME
    api_version: "2024-02-15-preview"
    timeout: 30  # Timeout in seconds
    max_retries: 3
    rate_limiting:
      limit: 100
      window_secs: 3600  # 1 hour
      enabled: true

  # Ollama configuration (for local models)
  ollama:
    base_url: "http://host.docker.internal:11434/v1"
    timeout_seconds: 1200
    rate_limiting:
      limit: 500
      window_secs: 3600  # 1 hour
      enabled: true

  # MiniMax configuration
  minimax:
    api_key: "${MINIMAX_API_KEY}"
    base_url: "https://api.minimax.io/anthropic"
    timeout_seconds: 30
    rate_limiting:
      limit: 100
      window_secs: 3600  # 1 hour
      enabled: true

  # OpenRouter configuration
  openrouter:
    api_key: "${OPENROUTER_API_KEY}"
    base_url: "https://openrouter.ai/api/v1"
    rate_limiting:
      limit: 150
      window_secs: 3600  # 1 hour
      enabled: true

  # Together AI configuration
  together_ai:
    api_key: "${TOGETHER_API_KEY}"
    rate_limiting:
      limit: 120
      window_secs: 3600  # 1 hour
      enabled: true

  # Replicate configuration
  replicate:
    api_key: "${REPLICATE_API_KEY}"
    rate_limiting:
      limit: 100
      window_secs: 3600  # 1 hour
      enabled: true

# Sink system configuration
sinks:
  # Global sink settings
  global:
    enabled: true
    # default_sink: "memory"
    default_sink: "loki"

    # Buffer settings
    buffer:
      type: "memory"
      max_size: 524288 # 512KB (reduced from 2MB)
      max_events: 100 # reduced from 500
      backpressure_threshold: 0.7

    # Batch settings
    batch:
      max_events: 50 # reduced from 100
      max_bytes: 131072 # 128KB (reduced from 256KB)
      timeout_secs: 5 # reduced from 10 for faster processing
      compression: "gzip" # Options: none, gzip, zstd, snappy
      partition_strategy: "none" # Options: none, by_time, by_provider, by_model, by_fields, by_request_id
      # partition_fields: []  # Required when partition_strategy is "by_fields"

    # Retry settings
    retry:
      initial_backoff_ms: 100
      max_backoff_ms: 10000 # reduced from 30000
      max_retries: 3 # reduced from 10
      backoff_factor: 2.0
      retry_transient_only: true

    # Acknowledgements settings
    acknowledgements:
      enabled: true
      timeout_secs: 15 # reduced from 30
      mode: "batch" # Options: individual, batch
      max_pending_events: 1000 # reduced from 10000

    # Background task management
    max_idle_cycles: 1 # Maximum idle cycles before stopping background tasks

  # Individual sink configurations
  sinks:
    # Memory Sink (for testing and development)
    # memory:
    #   sink_type: "memory"
    #   storage_approach: "combined" # Options: combined, separate, hybrid, default
    #   config:
    #     max_events: 100000
    #     persist_on_shutdown: false
    #     # persist_path: "/tmp/myway-memory-sink.json"

    # OpenSearch Sink - Configurable Storage Approach
    opensearch:
      sink_type: "elasticsearch"
      storage_approach: "separate" # Options: "combined", "separate", "hybrid", "default"
      config:
        url: "http://opensearch:9200"
        # username: "elastic"
        # password: "changeme"
        index: "llm-logs"
        use_bulk: true
        opensearch: true
        headers:
          "User-Agent": "myway/1.0"

        # Storage behavior configuration
        storage_mode: "separate" # Options: "combined" | "separate"

        # Combined mode options (when storage_mode: "combined")
        combined_config:
          combine_timeout_ms: 5000 # Wait time for response before storing incomplete request
          single_document_structure: true # Store request+response in one document
          include_request_body: true # Include full request data
          include_response_body: true # Include full response data
          flatten_nested_fields: false # Keep nested structure vs flatten

        # Separate mode options (when storage_mode: "separate")
        separate_config:
          add_doc_type_field: true # Add doc_type: "request"|"response" field
          use_same_index: true # Use same index for both request/response
          request_index_suffix: "" # Optional: "-requests" for separate indices
          response_index_suffix: "" # Optional: "-responses" for separate indices
          include_correlation_id: true # Include request_id for linking docs
          store_request_immediately: true # Store request before waiting for response

    # # PostgreSQL Sink
    # postgres:
    #   sink_type: "postgres"
    #   storage_approach: "separate"
    #   config:
    #     connection_string: "postgresql://postgres:postgres@postgres:5432/myway"
    #     schema: "myway"
    #     requests_table: "llm_requests"
    #     responses_table: "llm_responses"
    #     entries_table: "llm_entries"
    #     pool_size: 10
    #     create_tables: true

    # # ClickHouse Sink - Configurable Storage Approach (Analytics & OLAP)
    # clickhouse:
    #   sink_type: "clickhouse"
    #   storage_approach: "separate" # Options: "combined", "separate", "hybrid", "default"
    #   config:
    #     url: "http://clickhouse:8123"
    #     username: "default"
    #     password: "clickhouse123"
    #     database: "myway"
    #     create_tables: true

    #     # Storage behavior configuration
    #     storage_mode: "separate" # Options: "combined" | "separate"

    #     # Combined mode options (when storage_mode: "combined")
    #     combined_config:
    #       table_name: "llm_events" # Single table for request+response
    #       include_request_data: true # Include full request payload
    #       include_response_data: true # Include full response payload
    #       partition_by: "toYYYYMM(timestamp)" # Monthly partitions
    #       order_by: "(timestamp, provider, model)" # Optimize for time-series queries
    #       ttl_expression: "timestamp + INTERVAL 1 YEAR" # Data retention

    #     # Separate mode options (when storage_mode: "separate")
    #     separate_config:
    #       requests_table: "llm_requests" # Request-specific table
    #       responses_table: "llm_responses" # Response-specific table
    #       use_materialized_views: true # Create analytics views
    #       partition_by: "toYYYYMM(timestamp)" # Monthly partitions
    #       order_by: "(timestamp, request_id)" # Optimize for correlation
    #       ttl_expression: "timestamp + INTERVAL 1 YEAR" # Data retention

    #     # Analytics optimization
    #     analytics_config:
    #       enable_compression: true # Use ClickHouse compression
    #       compression_codec: "ZSTD(1)" # Compression algorithm
    #       enable_aggregations: true # Pre-compute common metrics
    #       create_summary_tables: true # Create hourly/daily summaries
    #       enable_token_analytics: true # Track token usage patterns

    # # AWS S3 Sink (for archival) - TEMPORARILY DISABLED
    # # s3:
    # #   sink_type: "s3"
    # #   storage_approach: "combined"
    # #   config:
    # #     region: "us-east-1"
    # #     bucket: "myway-llm-data"
    # #     prefix: "requests"
    # #     # access_key_id: "${AWS_ACCESS_KEY_ID}"

    # # MySQL Sink (for relational data storage)
    # mysql:
    #   sink_type: "mysql"
    #   storage_approach: "separate"
    #   config:
    #     connection_string: "mysql://myway:password@mysql:3306/myway"
    #     database: "myway"
    #     requests_table: "llm_requests"
    #     responses_table: "llm_responses"
    #     entries_table: "llm_entries"
    #     pool_size: 10
    #     create_tables: true

    #     # MySQL-specific optimizations
    #     optimization_config:
    #       enable_query_cache: true
    #       connection_timeout_seconds: 30
    #       read_timeout_seconds: 60
    #       write_timeout_seconds: 60
    #       max_packet_size: 67108864 # 64MB

    #     # Analytics features
    #     analytics_config:
    #       enable_daily_summary_view: true
    #       enable_error_analysis_view: true
    #       enable_performance_monitoring: true
    #       create_cost_tracking: true

    # # Kafka Sink (for real-time streaming and analytics)
    # kafka:
    #   sink_type: "kafka"
    #   storage_approach: "separate"
    #   config:
    #     brokers: "kafka:29092"
    #     topic: "llm-events"
    #     request_topic: "llm-requests"
    #     response_topic: "llm-responses"

    #     # Authentication (optional)
    #     # username: "${KAFKA_USERNAME}"
    #     # password: "${KAFKA_PASSWORD}"
    #     security_protocol: "plaintext"
    #     # sasl_mechanism: "PLAIN"  # Options: PLAIN, SCRAM-SHA-256, SCRAM-SHA-512

    #     # Producer configuration
    #     acks: "all" # Options: "0", "1", "all"
    #     compression_type: "snappy" # Options: none, gzip, snappy, lz4, zstd
    #     request_timeout_secs: 30
    #     retries: 3

    #     # Performance tuning
    #     additional_config:
    #       "batch.size": "16384"
    #       "linger.ms": "10"
    #       "delivery.timeout.ms": "300000" # 5 minutes

    #     # Message formatting
    #     include_metadata: true
    #     key_format: "{provider}-{model}-{id}"

    #     # Topic management
    #     auto_create_topics: true
    #     partitions: 3
    #     replication_factor: 1

    #     # Streaming configuration
    #     streaming_config:
    #       enable_request_streaming: true
    #       enable_response_streaming: true
    #       include_headers: true
    #       header_prefix: "myway-"
    #       timestamp_format: "iso8601"

    # # MinIO Sink (S3-compatible object storage)
    # minio:
    #   sink_type: "minio"
    #   storage_approach: "combined"
    #   config:
    #     url: "http://minio:9000"
    #     bucket: "myway-llm-events"
    #     prefix: "llm-events"
    #     access_key: "minioadmin"
    #     secret_key: "minioadmin"
    #     path_style: true
    #     key_format: "llm/{provider}/{model}/%Y/%m/%d/%H/{uuid}.json"
    #     use_uuid_prefix: true # Set to true to add UUID prefixes to object keys

    #     # Security configuration
    #     # For production, use environment variables:
    #     # access_key: "${MINIO_ACCESS_KEY}"
    #     # secret_key: "${MINIO_SECRET_KEY}"

    #     # Optional SSL configuration
    #     # ssl_enabled: true
    #     # ssl_cert_path: "/path/to/cert.pem"
    #     # ssl_key_path: "/path/to/key.pem"

    #     # Storage optimization
    #     storage_config:
    #       content_type: "application/json"
    #       server_side_encryption: false
    #       multipart_threshold: 16777216 # 16MB
    #       max_upload_parts: 10000
    #       part_size: 5242880 # 5MB

    #     # Lifecycle management
    #     lifecycle_config:
    #       enable_versioning: false
    #       retention_days: 365
    #       delete_incomplete_uploads_days: 7
    #       transition_to_ia_days: 30 # Transition to Infrequent Access after 30 days

    # Loki Sink (Grafana Loki log aggregation)
    loki:
      sink_type: "loki"
      storage_approach: "combined"
      config:
        endpoint: "http://loki:3100"
        tenant_id: null
        username: null
        password: null
        bearer_token: null
        headers: {}
        timeout_seconds: 30
        batch_size: 50 # reduced from 1000
        batch_timeout_ms: 500 # reduced from 1000
        remove_timestamp: false
        compression: true
        labels:
          environment: "development"
          service: "myway-llm"
          source: "llm-gateway"
        label_template: "provider={provider},model={model},status={status}"
        include_request_labels: true
        include_response_labels: true
        max_retries: 2 # reduced from 3
        retry_delay_ms: 100
        tls_verify: true
        ca_cert_file: null
        client_cert_file: null
        client_key_file: null

  # Event routing configuration
  routing:
    # Default sinks for ALL events - OpenSearch for search/visualization, ClickHouse for analytics, PostgreSQL for structured storage, Kafka for real-time streaming, Loki for logs, Doris for real-time analytics
    default_sinks:
      # ["opensearch", "clickhouse", "postgres", "kafka", "minio", "loki"]
      ["loki"]

    # Routing rules (processed in order) - these are additional to defaults
    rules:
      # Analytics-focused routing - all providers go to ClickHouse for OLAP analysis

      # High-volume providers - comprehensive analytics and streaming
      - provider: "openai"
        sinks: [
            "opensearch",
            # "s3",
            # "clickhouse",
            # "postgres",
            # "mysql",
            # "kafka",
            # "minio",
            "loki",
            # "doris",
          ]

      # Anthropic requests - search, analytics, postgres and streaming
      - provider: "anthropic"
        sinks: [
            # "opensearch",
            # "clickhouse",
            # "postgres",
            # "mysql",
            # "kafka",
            # "minio",
            "loki",
            # "doris",
          ]

      # Embedding requests - lightweight storage and streaming
      # - request_type: "embedding"
      #   sinks: ["opensearch", "clickhouse", "kafka", "minio", "loki"]

      # Failed requests - full analysis pipeline with streaming for alerting
      - success: false
        sinks:
          # ["opensearch", "clickhouse", "postgres", "mysql", "kafka", "minio"]
          ["loki"]

      # Successful requests - include logs in Loki for monitoring
      - success: true
        sinks: [
            "opensearch",
            # "s3",
            # "clickhouse",
            # "postgres",
            # "mysql",
            # "kafka",
            # "minio",
            "loki",
            # "doris",
          ]

      # High token usage - deep analytics and real-time monitoring
      - field: "usage.total_tokens"
        operator: "gt"
        value: 1000
        sinks:
          # ["opensearch", "clickhouse", "postgres", "mysql", "kafka", "minio"]
          ["loki"]

      # Expensive models - track costs with real-time alerts
      - model: "gpt-4"
        sinks:
          # ["opensearch", "clickhouse", "postgres", "mysql", "kafka", "minio"]
          ["loki"]

      # Streaming requests - real-time analytics and Kafka streaming
      # - field: "stream"
      #   operator: "eq"
      #   value: true
      #   sinks: ["opensearch", "clickhouse", "kafka", "minio"]

      # Large response data - archive to object storage
      # - field: "response.usage.total_tokens"
      #   operator: "gt"
      #   value: 2000
      #   sinks: ["minio", "clickhouse"]

      # Long conversation history - archive to MinIO for cost-effective storage
      # - field: "request.messages"
      #   operator: "array_length_gt"
      #   value: 10
      #   sinks: ["minio", "opensearch"]

# Environment-specific overrides
environments:
  development:
    global:
      logging:
        level: "debug"
        format: "pretty"
      server:
        port: 3000
      rate_limiting:
        enabled: true
        # backend: "memory"
        backend: "redis"
        redis:
          url: "${REDIS_URL:-redis://localhost:6379}"
          key_prefix: "myway:prod:rate_limit:"
          connection_timeout: 10
          command_timeout: 5
        routes:
          - route: "/v1/chat/completions"
            limit: 200
            window_secs: 3600
            enabled: true
          - route: "/v1/embeddings"
            limit: 1000
            window_secs: 3600
            enabled: true
          - route: "/v1/models"
            limit: 2000
            window_secs: 3600
            enabled: true
        default:
          limit: 500
          window_secs: 3600
          enabled: true

    providers:
      default_provider: "openai"
      timeout_seconds: 300

    sinks:
      global:
        enabled: true
        # default_sink: "memory"
        default_sink: "loki"
        buffer:
          type: "memory"
          max_size: 1048576 # 1MB (reduced from 100MB for memory optimization)
          max_events: 100 # reduced from 10000
          backpressure_threshold: 0.9
        batch:
          max_events: 10 # reduced from 50
          max_bytes: 32768 # 32KB (reduced from 128KB)
          timeout_secs: 2 # reduced from 5
          compression: "gzip"
          partition_strategy: "none"
        retry:
          initial_backoff_ms: 100
          max_backoff_ms: 30000
          max_retries: 10
          backoff_factor: 2.0
          retry_transient_only: true
        acknowledgements:
          enabled: true
          timeout_secs: 30
          mode: "batch"
          max_pending_events: 10000

        # Background task management
        max_idle_cycles: 5 # Reduced for development environment
      sinks:
        # memory:
        #   sink_type: "memory"
        #   config:
        #     max_events: 1000
        loki:
          sink_type: "loki"
          config:
            endpoint: "http://loki:3100"
            labels:
              environment: "development"
            batch_size: 100
            batch_timeout_ms: 1000

        # AWS S3 Sink (for archival) - TEMPORARILY DISABLED
        # s3:
        #   sink_type: "s3"
        #   storage_approach: "combined"
        #   config:
        #     region: "${AWS_DEFAULT_REGION:-us-west-1}"
        #     bucket: "${S3_BUCKET:-myway-llm-data}"
        #     prefix: "events"
        #     storage_class: "STANDARD_IA"
        #     access_key_id: "${AWS_ACCESS_KEY_ID}"
        #     secret_access_key: "${AWS_SECRET_ACCESS_KEY}"
        #     session_token: "${AWS_SESSION_TOKEN}"
        #     compression: "gzip"
        #     batch_size: 100
        #     batch_timeout_ms: 5000
